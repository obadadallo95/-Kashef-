import requests
import json
from datetime import datetime
import config

def run_collection():
    print("üöÄ [Step 1] Starting Data Collection...")
    
    # Categorized Seed Data (Massive Injection)
    seed_data = {
        # üî¥ STRICT BAN (Criminal, Drugs, Severe Abuse) - No AI Check needed
        "strict_ban": {
            "drugs": [
                "ŸÉÿ®ÿ™ÿßÿ∫ŸàŸÜ", "ŸÉÿ®ÿ™ÿßÿ¨ŸàŸÜ", "ŸÑŸÉÿ≤ÿ≥", "ÿ£ÿ®Ÿà ŸáŸÑÿßŸÑŸäŸÜ", "ÿ≤Ÿáÿ±Ÿäÿ©", "ŸÉÿßÿ®ÿ™ŸÜÿß", "ŸÖÿÆÿØÿ±ÿßÿ™", 
                "ÿ≠ÿ¥Ÿäÿ¥", "ŸÖÿßÿ±Ÿäÿ¨ŸàÿßŸÜÿß", "ŸÉÿ±Ÿäÿ≥ÿ™ÿßŸÑ ŸÖŸäÿ´", "ÿ¥ÿ®Ÿà", "ŸáŸäÿ±ŸàŸäŸÜ", "ÿ®ŸàÿØÿ±ÿ©", "ÿ¥ŸÖ", 
                "ÿ™ÿ±ŸÖÿßŸÑ", "ÿ™ÿ±ÿßŸÖÿßÿØŸàŸÑ", "ÿ≤ŸàŸÑÿßŸÖ", "ÿ®Ÿäÿπ ŸÉŸÑŸâ", "ÿ®Ÿäÿπ ŸÉŸÑŸäÿ©", "ŸÉŸÑŸäÿ© ŸÑŸÑÿ®Ÿäÿπ"
            ],
            "weapons_market": [
                "ÿ®Ÿäÿπ ÿ≥ŸÑÿßÿ≠", "ÿ≥ŸÑÿßÿ≠ ŸÑŸÑÿ®Ÿäÿπ", "ÿ®ÿßÿ±ŸàÿØÿ© ŸÑŸÑÿ®Ÿäÿπ", "ŸÖÿ≥ÿØÿ≥ ŸÑŸÑÿ®Ÿäÿπ", "ŸÇŸÜÿ®ŸÑÿ© ŸÑŸÑÿ®Ÿäÿπ", 
                "ÿ∞ÿÆŸäÿ±ÿ© ŸÑŸÑÿ®Ÿäÿπ", "ŸÖÿ∑ŸÑŸàÿ® ÿ≥ŸÑÿßÿ≠", "ŸÑŸÑÿ®Ÿäÿπ ŸÉŸÑÿßÿ¥ŸÜŸÉŸàŸÅ", "ŸÑŸÑÿ®Ÿäÿπ ÿ±Ÿàÿ≥Ÿäÿ©", "ÿ¨ŸÑŸàŸÉ ŸÑŸÑÿ®Ÿäÿπ"
            ],
            "sexual_harassment": [
                "ÿ¥ÿ±ŸÖŸàÿ∑ÿ©", "ŸÇÿ≠ÿ®ÿ©", "ŸÖŸÜŸäŸàŸÉÿ©", "ÿπÿßŸáÿ±ÿ©", "ŸÉÿ≥ ÿßÿÆÿ™ŸÉ", "ÿ∑Ÿäÿ≤", "ŸÅÿ≠ŸÑ", "ÿØŸäŸàÿ´", 
                "ŸÇÿ±ŸÜ", "ÿ≥ŸÉÿ≥", "ŸÖÿ≠ÿßÿ±ŸÖ", "ÿ≥ÿ≠ÿßŸÇ", "ŸÜŸäŸÉ", "ŸÖŸÖÿ≠ŸàŸÜÿ©", "ÿ®ŸÜÿ™ ŸÑŸäŸÑ", "ÿØÿπÿßÿ±ÿ©"
            ],
            "severe_insults": [
                "ŸÉÿ≥ ÿßŸÖŸÉ", "Ÿäÿß ÿßÿ®ŸÜ ÿßŸÑŸÇÿ≠ÿ®ÿ©", "Ÿäÿß ÿßÿ®ŸÜ ÿßŸÑÿ¥ÿ±ŸÖŸàÿ∑ÿ©", "Ÿäÿß ÿßÿ®ŸÜ ÿßŸÑÿ≠ÿ±ÿßŸÖ", "ŸÑÿπŸÜ ÿßŸÑŸÑŸá ÿØŸäŸÜŸÉ"
            ]
        },

        # üü° CONTEXTUAL WATCHLIST (Politics, Sectarianism, Grey Area) - Needs AI Check
        "contextual_watchlist": {
            "political_entities": [
                "ÿØÿßÿπÿ¥", "ÿßŸÑÿØŸàŸÑÿ© ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäÿ©", "ÿßŸÑŸÜÿµÿ±ÿ©", "ŸáŸäÿ¶ÿ© ÿ™ÿ≠ÿ±Ÿäÿ± ÿßŸÑÿ¥ÿßŸÖ", "ÿßŸÑÿ¨ŸàŸÑÿßŸÜŸä", 
                "ŸÇÿ≥ÿØ", "PYD", "YPG", "PKK", "ÿßŸÑÿ¨Ÿäÿ¥ ÿßŸÑÿ≠ÿ±", "ÿßŸÑÿ¨Ÿäÿ¥ ÿßŸÑŸàÿ∑ŸÜŸä", "ÿßŸÑÿßÿ¶ÿ™ŸÑÿßŸÅ", 
                "ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿ≥Ÿàÿ±Ÿä", "ÿßŸÑÿ£ÿ≥ÿØ", "ŸÖÿßŸáÿ± ÿßŸÑÿ£ÿ≥ÿØ", "ÿ≠ÿ≤ÿ® ÿßŸÑŸÑŸá", "ÿ≠ÿ≥ŸÜ ŸÜÿµÿ±ÿßŸÑŸÑŸá", 
                "ÿ•Ÿäÿ±ÿßŸÜ", "ÿ±Ÿàÿ≥Ÿäÿß", "ÿßŸÑÿßÿ≠ÿ™ŸÑÿßŸÑ", "ÿßŸÑÿµŸáŸäŸàŸÜŸä", "ÿßŸÑŸÖŸàÿ≥ÿßÿØ", "ÿßŸÑŸÖÿÆÿßÿ®ÿ±ÿßÿ™"
            ],
            "sectarian_slurs": [
                "ŸÜÿµŸäÿ±Ÿä", "ÿπŸÑŸàŸäÿ©", "ÿ±ÿßŸÅÿ∂Ÿä", "ÿ±ŸàÿßŸÅÿ∂", "ŸÖÿ¨Ÿàÿ≥", "ÿµŸÅŸàŸä", "ÿ≥ŸÜŸä", "ŸÜŸàÿßÿµÿ®", 
                "ŸÖÿ±ÿ™ÿØ", "ŸÉÿßŸÅÿ±", "ŸÖÿ¥ÿ±ŸÉ", "ÿ∞ŸÖŸä", "ÿµŸÑŸäÿ®Ÿä", "ÿØÿ±ÿ≤Ÿä", "ŸÇÿ±ŸàÿØ", "ÿÆŸÜÿßÿ≤Ÿäÿ±"
            ],
            "violence_terms": [
                "ÿ∞ÿ®ÿ≠", "ŸÇÿ™ŸÑ", "ŸÜÿ≠ÿ±", "ÿØÿπÿ≥", "ŸÅÿ∑ÿ≥", "ŸÜŸÅŸÇ", "ÿ™ÿµŸÅŸäÿ©", "ÿ•ÿπÿØÿßŸÖ", "ÿ™ŸÅÿ¨Ÿäÿ±", 
                "ÿßŸÜÿ™ÿ≠ÿßÿ±Ÿä", "ÿßÿ≥ÿ™ÿ¥ŸáÿßÿØŸä", "ŸÖŸÅÿÆÿÆÿ©", "ÿπÿ®Ÿàÿ©", "ÿµÿßÿ±ŸàÿÆ", "ÿ®ÿ±ŸÖŸäŸÑ", "ŸÉŸäŸÖÿßŸàŸä"
            ],
            "controversial_symbols": [
                "üîª", "üçâ", "üí£", "üí•", "üî™", "ü©∏", "ü´°", "ü•∑", "üè¥", "üè≥Ô∏è"
            ],
            "bullying": [
                "ŸÖÿπÿßŸÇ", "ŸÖŸÜÿ∫ŸàŸÑŸä", "ÿ™Ÿàÿ≠ÿØ", "ÿ£Ÿáÿ®ŸÑ", "ÿ¨ÿ≠ÿ¥", "ÿ≠ŸäŸàÿßŸÜ", "ÿ∫ÿ®Ÿä", "ŸÖÿ™ÿÆŸÑŸÅ", 
                "ÿ≥ŸÖŸäŸÜ", "ÿØÿ®ÿ©", "ŸÅŸäŸÑ", "ŸÇÿ≤ŸÖ", "ÿ®ÿ±ÿ∫Ÿä", "ŸÜŸàÿ±Ÿä", "ŸÇÿ±ÿ®ÿßÿ∑Ÿä", "ÿ¥ÿ±ÿ¥Ÿàÿ≠"
            ],
            "scams": [
                "ÿØŸàŸÑÿßÿ± ŸÖÿ¨ŸÖÿØ", "ÿØŸàŸÑÿßÿ± ŸÑŸäÿ®Ÿä", "ŸäŸàÿ±Ÿà ŸÖÿ¨ŸÖÿØ", "ÿ™Ÿáÿ±Ÿäÿ®", "ÿ∑ÿ±ŸäŸÇ ÿßŸÑŸäŸàŸÜÿßŸÜ", 
                "ŸÑŸÖ ÿ¥ŸÖŸÑ ŸÖÿ∂ŸÖŸàŸÜ", "ÿ¨Ÿàÿßÿ≤ ÿ≥ŸÅÿ±", "ŸÅŸäÿ≤ÿß ŸÖÿ∂ŸÖŸàŸÜÿ©", "ŸÅŸàÿ±ŸÉÿ≥", "ÿ™ÿØÿßŸàŸÑ"
            ]
        }
    }

    collected_words = set(config.SEED_WORDS)
    
    # Flatten seed data for processing (Handles 2 levels of nesting)
    for category, subcategories in seed_data.items():
        if isinstance(subcategories, dict):
            for subcat, words in subcategories.items():
                for w in words:
                    collected_words.add(w)
        elif isinstance(subcategories, list):
             for w in subcategories:
                 collected_words.add(w)


    print(f"   ‚úÖ Loaded massive seed dataset ({len(collected_words)} terms).")

    # Fetch from Sources
    for source in config.SOURCES:
        try:
            print(f"   Downloading {source['name']}...")
            try:
                response = requests.get(source['url'], timeout=10)
                if response.status_code == 200:
                    lines = response.text.split('\n')
                    count = 0
                    for line in lines:
                        parts = line.split('\t')
                        if len(parts) > 1:
                            text = parts[source['column_text']].strip()
                            label = parts[source['column_label']].strip()
                            
                            if label == source['hate_label']:
                                words = text.split()
                                for w in words:
                                    if len(w) > 2: 
                                        collected_words.add(w)
                                count += 1
                    print(f"   ‚úÖ Processed {count} hate samples from source.")
                else:
                    print(f"   ‚ùå Failed to download {source['name']} (Status: {response.status_code})")
            except requests.exceptions.RequestException as e:
                print(f"   ‚ùå Network error calling source: {e}")

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error collecting from {source['name']}: {e}")

    # Save to JSON
    data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source": "Kashef Data Factory (Sources + Categorized Seed)"
        },
        "words": list(collected_words)
    }

    with open(config.RAW_DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Step 1A Complete. Saved {len(collected_words)} unique terms to {config.RAW_DATA_FILE}")

def collect_official_policies():
    print("ü§ñ [Step 1B] Collecting Official Policy Violations (Facebook/X)...")
    import ai_generator # Lazy import
    
    generator = ai_generator.AIWordsGenerator()
    
    # Official Policies to scrape via AI
    policy_targets = [
        ("Facebook", "Dangerous Individuals & Organizations"),
        ("Facebook", "Bullying & Harassment"),
        ("Facebook", "Regulated Goods (Drugs/Weapons)"),
        ("Twitter", "Hateful Conduct")
    ]
    
    new_terms = set()
    
    for platform, policy in policy_targets:
        print(f"   üìú Extracting violations from {platform} for '{policy}'...")
        try:
            json_str = generator.extract_policy_terms(platform, policy)
            # Try to parse JSON
            try:
                parsed = json.loads(json_str)
                terms = []
                if 'policy_terms' in parsed:
                    terms = parsed['policy_terms']
                elif isinstance(parsed, list):
                    terms = parsed
                else:
                    for v in parsed.values():
                        if isinstance(v, list):
                            terms = v
                            break
                
                valid_count = 0
                for t in terms:
                    if isinstance(t, str) and len(t) > 1:
                        new_terms.add(t)
                        valid_count += 1
                        
                print(f"      Found {valid_count} terms.")
            except json.JSONDecodeError:
                print(f"      ‚ö†Ô∏è Failed to parse JSON response.")
                
        except Exception as e:
            print(f"      ‚ö†Ô∏è Failed to extract for {policy}: {e}")

    # Load and Merge
    try:
        with open(config.RAW_DATA_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            current_words = set(data['words'])
    except FileNotFoundError:
        current_words = set()

    before_count = len(current_words)
    current_words.update(new_terms)
    after_count = len(current_words)
    
    print(f"   ‚ú® Policy AI added {after_count - before_count} new terms.")

    data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source": "Kashef Data Factory"
        },
        "words": list(current_words)
    }

    with open(config.RAW_DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    run_collection()
    collect_official_policies()
